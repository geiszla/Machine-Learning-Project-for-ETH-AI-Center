"""Module for fine-tuning a reward model using preference data."""
import math
import pickle
from os import path
from pathlib import Path

import numpy as np
import torch
import torch.optim as optim
from network import Network
from torch.utils.data import DataLoader, Dataset, random_split


class PreferenceDataset(Dataset):
    """PyTorch Dataset for loading preference data."""

    def __init__(self, file_path):
        """Initialize dataset."""
        with open(file_path, "rb") as handle:
            self.pairs_of_trajectories = pickle.load(handle)
        # TODO: initialize data: data consists of tuples of observations
        #  (maybe also rewards, but not needed here), sorted by preference
        # dataset is created by reward_model/create_preference_dataset.py
        # which depends on output/preferences.csv and the data generated by rl/generate_videos_and_data.py
        # Note: current file observation_data_corresponding_to_videos/ppo_HalfCheetah-v3_obs_reward_dataset.pkl was generated by previous version
        # of rl/generate_videos_and_data.py which included rewards

    def __len__(self):
        """Return size of dataset."""
        pass
        # TODO: return length of dataset

    def __getitem__(self, idx):
        """Return item with given index."""
        pass
        # TODO. return item


def compute_loss(batch, model, device):
    """Compute the loss for a batch of data."""
    traj0_batch = torch.stack([traj0.clone().detach() for traj0, _ in batch]).to(device)
    traj1_batch = torch.stack([traj1.clone().detach() for _, traj1 in batch]).to(device)

    rewards0 = model(traj0_batch).sum(dim=1)
    rewards1 = model(traj1_batch).sum(dim=1)

    probs_softmax = torch.exp(rewards0) / (torch.exp(rewards0) + torch.exp(rewards1))
    loss = -torch.sum(torch.log(probs_softmax))
    return probs_softmax, loss


def train_reward_model(
    reward_model: Network,
    dataset: Dataset,
    epochs: int,
    batch_size: int,
    split_ratio: float = 0.8,
    patience: int = 10,
):
    """Train a reward model given preference data."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    reward_model = reward_model.to(device)
    optimizer = optim.Adam(reward_model.parameters(), lr=1e-4)

    train_size = math.floor(split_ratio * len(dataset))
    train_set, val_set = random_split(
        dataset, lengths=[train_size, len(dataset) - train_size]
    )

    train_loader = DataLoader(
        train_set, batch_size=batch_size, shuffle=True, pin_memory=True
    )
    val_loader = DataLoader(
        val_set, batch_size=batch_size, shuffle=True, pin_memory=True
    )

    best_val_loss = float("inf")
    no_improvement_epochs = 0

    best_model_state = reward_model.state_dict()  # Initialize with the initial state
    for epoch in range(epochs):
        # Training
        train_losses = []
        for batch in train_loader:
            _probs_softmax_, loss = compute_loss(batch, reward_model, device)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())
        avg_train_loss = np.mean(train_losses)

        # Validation
        with torch.no_grad():
            val_losses = []
            for obs, reward in val_loader:
                _, loss = compute_loss(batch, reward_model, device)
                val_losses.append(loss.item())
            avg_val_loss = np.mean(val_losses)

        print(
            f"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}"
        )

        # Early stopping
        delta = 0.001  # minimum acceptable improvement
        if avg_val_loss < best_val_loss - delta:
            best_val_loss = avg_val_loss
            best_model_state = (
                reward_model.state_dict()
            )  # save the parameters of the best model
            current_path = Path(__file__).parent.resolve()
            torch.save(
                best_model_state,
                path.join(current_path, "models", "reward_model_finetuned.pth"),
            )  # save the parameters for later use to disk
            no_improvement_epochs = 0
        else:
            no_improvement_epochs += 1
            if no_improvement_epochs >= patience:
                print(
                    f"No improvement after for {patience} epochs, therefore stopping training."
                )
                break  # break instead of return, so that the function can return the best model state

    # load the weights and biases for the best model state during training
    reward_model.load_state_dict(best_model_state)

    return reward_model


def main():
    """Run reward model fine-tuning."""
    # File paths
    current_path = Path(__file__).parent.resolve()
    folder_path = path.join(current_path, "../rl/reward_data")
    file_path = path.join(folder_path, "ppo_HalfCheetah-v3_obs_reward_dataset.pkl")

    # Load data
    dataset = PreferenceDataset(file_path)

    # Initialize network
    reward_model = Network(layer_num=3, input_dim=17, hidden_dim=256, output_dim=1)

    # Train model
    train_reward_model(reward_model, dataset, epochs=100, batch_size=32)


if __name__ == "__main__":
    main()
